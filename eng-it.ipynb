{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62ef9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import spacy\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "print(torch.__version__)\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c0435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 256\n",
    "MAX_SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 32\n",
    "Nx=3\n",
    "N_HEADS = 4\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8e60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en\n",
    "!python3 -m spacy download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9291d6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed in minutes, 0.4317539652188619\n",
      "time elapsed in seconds after Tab dataset 4.983298150698344\n",
      "Unique tokens in source (it) vocabulary: 10004\n",
      "Unique tokens in target (en) vocabulary: 10004\n",
      "time elapsed in seconds after build_vocab + BucketIterator 0.23890841404596966\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "spacy_italian = spacy.load(\"it_core_news_sm\")\n",
    "spacy_english = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_italian(text):\n",
    "    return [token.text for token in spacy_italian.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)]\n",
    "\n",
    "italian = Field(tokenize=tokenize_italian, lower=True,\n",
    "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "english = Field(tokenize=tokenize_english, lower=True,\n",
    "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eng_file_path = \"/Users/stephone_christian/Downloads/it-en/europarl-v7.it-en.en\"\n",
    "it_file_path = \"/Users/stephone_christian/Downloads/it-en/europarl-v7.it-en.it\"\n",
    "\n",
    "europarl_en = open(eng_file_path, encoding='utf-8').read().split('\\n')\n",
    "europarl_it = open(it_file_path, encoding='utf-8').read().split('\\n')\n",
    "\n",
    "\n",
    "raw_data = {'English' : [line for line in europarl_en], 'Italian': [line for line in europarl_it]}\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"Italian\"])\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "fields = [('English', english), ('Italian', italian)]\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"/Users/stephone_christian/Downloads/it-en/data\"\n",
    "train.to_csv(data_path + \"/train.csv\", index=False)\n",
    "test.to_csv(data_path + \"/test.csv\", index=False)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'time elapsed in minutes, {(t1 - t0) / 60}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data, test_data = TabularDataset.splits( path=data_path,\n",
    "                                 train=\"train.csv\",\n",
    "                                 test=\"test.csv\",\n",
    "                                 format=\"csv\",\n",
    "                                 fields=fields,\n",
    "                                 skip_header=True)\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "print(f'time elapsed in seconds after Tab dataset {(t2 - t1) / 60}')\n",
    "\n",
    "italian.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "\n",
    "#bucketIterator\n",
    "train_iterator, test_iterator = BucketIterator.splits((train_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE, \n",
    "                                                                      sort_within_batch=True,\n",
    "                                                                      sort_key=lambda x: len(x.Italian))\n",
    "\n",
    "\n",
    "print(f\"Unique tokens in source (it) vocabulary: {len(italian.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(english.vocab)}\")\n",
    "\n",
    "\n",
    "t3 = time.time()\n",
    "print(f'time elapsed in seconds after build_vocab + BucketIterator {(t3 - t2) / 60}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324a31bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 32])\n"
     ]
    }
   ],
   "source": [
    "#let's just use spacy and use a preprocessed dataset. In future I can come back\n",
    "#train_iterator.create_batches()\n",
    "for x in train_iterator:\n",
    "    \n",
    "    print(x.English.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7f87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PositionalEncoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_length,d_model, dtype=torch.float32)\n",
    "        for pos in range(max_length):\n",
    "            for i in range(0, d_model-1, 2):\n",
    "                pe[pos, i] = math.sin(pos / (float(10000)**(2*i / d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (float(10000)**(2*i / d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "                \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x *= math.sqrt(self.d_model)\n",
    "        x += self.pe[:,:x.shape[1]]\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#embedding layer\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "        \n",
    "#layerNormalization add and norm ( x = x + sublayer)\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d_model=D_MODEL):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        \n",
    "        self.eps = 1e-6\n",
    "        self.size = d_model\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_mean = x.mean(dim=-1, keepdim =True)\n",
    "        x_std = x.std(dim=-1, keepdim=True)\n",
    "        \n",
    "        out = self.alpha* ((x - x_mean) / (x_std + self.eps)) + self.bias\n",
    "        return out\n",
    "        \n",
    "#blue subbox in the paper #check the sizing\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.fc_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc_2(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def attention(q, k, v, d_k,  mask=None, dropout=None):\n",
    "    \n",
    "    \n",
    "    #print(\"attention:\", q.shape)\n",
    "    #print(\"attention:\", k.transpose(-2, -1).shape)\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    #print(\"scores shape:\", scores.shape)\n",
    "    if mask != None:\n",
    "        #print(\"mask shape before unsqueeze:\",mask.shape)\n",
    "        mask = mask.unsqueeze(1)\n",
    "        #print(\"mask shape after unsqueeze:\", mask.shape)\n",
    "        #print(score.shape)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = torch.nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output  \n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, drop_out=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(drop_out) \n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.h = N_HEADS\n",
    "        self.d_k = d_model // N_HEADS\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0) #batch size\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.K(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.Q(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.V(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "# calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def model_blocks(module, N=Nx):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, drop_out=0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.norm_1 = LayerNormalization()\n",
    "        self.ff = FeedForward(d_model)\n",
    "        #self.pe = PositionalEncoding(d_model, MAX_SEQ_LEN)\n",
    "        self.attn = MultiHeadAttention(d_model, drop_out)\n",
    "        self.norm_2 = LayerNormalization()\n",
    "        self.drop_1 = nn.Dropout(drop_out)\n",
    "        self.drop_2 = nn.Dropout(drop_out)\n",
    "            \n",
    "    def forward(self, x, mask=None):\n",
    "        #forward(q, k, v, mask=None)\n",
    "        #print(\"MADE TO ENCODER!!!!!!!!!\")\n",
    "        x_attn = self.drop_1(self.attn(x, x, x, mask))\n",
    "        x_out_1 = self.norm_1(x+x_attn)\n",
    "        out = self.norm_2(self.drop_2(self.ff(x_out_1)) + x_out_1)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, drop_out=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = LayerNormalization()\n",
    "        self.norm_2 = LayerNormalization()\n",
    "        self.norm_3 = LayerNormalization()\n",
    "        self.ff     = FeedForward(d_model)\n",
    "        self.drop_1 = nn.Dropout(drop_out)\n",
    "        self.drop_2 = nn.Dropout(drop_out)\n",
    "        self.drop_3 = nn.Dropout(drop_out)\n",
    "        \n",
    "        self.att_1 = MultiHeadAttention(d_model)\n",
    "        self.att_2 = MultiHeadAttention(d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, e_outputs,  src_msk, trg_msk):\n",
    "        out = self.drop_1(self.att_1(x, x, x, trg_msk))\n",
    "        x2 = self.norm_1(x + out)\n",
    "        x3 = self.drop_2(self.att_2(x2, e_outputs, e_outputs, src_msk))\n",
    "        \n",
    "        out = self.norm_2(x3 + x2)\n",
    "        \n",
    "        x4 = self.drop_3(self.ff(out))\n",
    "        out = self.norm_3(x4 + out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.emb = EmbeddingLayer(d_model, vocab)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.dec_blocks = model_blocks(DecoderBlock(d_model), Nx)\n",
    "        \n",
    "        #self.norm_1 = LayerNormalization()\n",
    "        #self.softmax = nn.Softmax() #might not need this\n",
    "        \n",
    "        \n",
    "    def forward(self, src, e_outputs, src_msk, trg_msk):\n",
    "        x = self.emb(src)\n",
    "        x = self.pos(x)\n",
    "        \n",
    "        for layer in self.dec_blocks:\n",
    "            x = layer(x, e_outputs, src_msk, trg_msk) # add e_output to this\n",
    "            \n",
    "        #x = self.norm_1(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.emb = EmbeddingLayer(d_model, vocab)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.enc_blocks = model_blocks(EncoderBlock(d_model), Nx)\n",
    "        \n",
    "    def forward(self, src, msk):\n",
    "        out = self.emb(src)\n",
    "        out = self.pos(out)\n",
    "        \n",
    "        for layer in self.enc_blocks:\n",
    "            out = layer(out, msk)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, src_vocab_size)\n",
    "        self.decoder = Decoder(d_model, trg_vocab_size)\n",
    "        self.fc = nn.Linear(d_model, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_msk, trg_msk):\n",
    "        e_outputs = self.encoder(src, src_msk)\n",
    "        out = self.decoder(trg, e_outputs, src_msk, trg_msk)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64213461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create optimizer\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "src_vocab_size = len(english.vocab)\n",
    "trg_vocab_size = len(italian.vocab)\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, D_MODEL)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, betas=(0.9, 0.98), eps= 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "#initialize parameters with xavier initialization\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97c6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_src_mask(src):\n",
    "    input_pad = english.vocab.stoi['<pad>']\n",
    "    # creates mask with 0s wherever there is padding in the input\n",
    "    src_msk = (src != input_pad).unsqueeze(1)\n",
    "    \n",
    "    return src_msk\n",
    "\n",
    "\n",
    "def create_trg_mask(trg):\n",
    "    input_pad = italian.vocab.stoi['<pad>']\n",
    "    trg_msk = (trg != input_pad).unsqueeze(1)\n",
    "    #print(\"trg_mask shape\", trg_msk.shape)\n",
    "    \n",
    "    size = trg.size(1) # get seq_len for matrix\n",
    "    #print(\"trg size\", size)\n",
    "    nopeak_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1)\n",
    "\n",
    "    nopeak_mask = torch.from_numpy(nopeak_mask) == 0\n",
    "    \n",
    "    trg_msk = trg_msk & nopeak_mask\n",
    "    return trg_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6700e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with a batch size of 32, there are ~62,500 updated per epoch :)\n",
    "model_path_root = \"/Users/stephone_christian/Downloads/it-en/data/model_checkpoints/\"\n",
    "loss_write_path = \"/Users/stephone_christian/Downloads/it-en/data/losses/loss.txt\"\n",
    "\n",
    "f = open(loss_write_path, \"w\")\n",
    "f.close() #clean the file\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = torch.load(model_path_root + \"3_30000\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(epoch)\n",
    "print(\"loss:\", loss)\n",
    "model.train()\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(EPOCH):\n",
    "    i = 0\n",
    "    losses = []\n",
    "    time_prev = time.time()\n",
    "    for data in train_iterator:\n",
    "        #losses.append(2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        src = data.English\n",
    "        trg = data.Italian\n",
    "        \n",
    "        #create the masks!\n",
    "        \n",
    "        \n",
    "        #reshape dimensions to fit the model\n",
    "        src = src.transpose(0,1) #transpose columns and rows, leave batch dimension alone\n",
    "        trg = trg.transpose(0, 1)\n",
    "        #print(\"src shape\", src.shape)\n",
    "        #print(\"trg shape\", trg.shape)\n",
    "        trg_input = trg[:,:-1]\n",
    "        targets = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        src_mask = create_src_mask(src)\n",
    "        #print(\"src mask shape\", src_mask.shape)\n",
    "        trg_mask = create_trg_mask(trg_input)\n",
    "        \n",
    "        y_pred = model(src, trg_input, src_mask, trg_mask)\n",
    "        \n",
    "        loss = criterion(y_pred.view(-1, y_pred.size(-1)), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % 100 == 0: #every 100 steps\n",
    "            #pass\n",
    "            print(f'step {i+1} of epoch {epoch}, loss: {loss.item():.6f}, elapsed time 1k steps (min) {((time.time() - time_prev) / 60):.5f}, total elapsed time (h) {((time.time() - t0) / 3600):.5f}')\n",
    "            time_prev = time.time()\n",
    "        \n",
    "        #if i % 1000 == 0:\n",
    "            \n",
    "                    \n",
    "        if i % 5000 == 0:\n",
    "            #checkpoint the model\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "            }, model_path_root + str(epoch+2) + \"_\" + str(i))\n",
    "        i +=1\n",
    "            \n",
    "        \n",
    "        \n",
    "    with open(loss_write_path, 'a') as f:\n",
    "        for l in losses:\n",
    "            f.write(str(l) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a01fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loss: 1.7252687215805054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): EmbeddingLayer(\n",
       "      (embedding): Embedding(10004, 256)\n",
       "    )\n",
       "    (pos): PositionalEncoding()\n",
       "    (enc_blocks): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (norm_1): LayerNormalization()\n",
       "        (ff): FeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (attn): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_2): LayerNormalization()\n",
       "        (drop_1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (norm_1): LayerNormalization()\n",
       "        (ff): FeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (attn): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_2): LayerNormalization()\n",
       "        (drop_1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (norm_1): LayerNormalization()\n",
       "        (ff): FeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (attn): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_2): LayerNormalization()\n",
       "        (drop_1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): EmbeddingLayer(\n",
       "      (embedding): Embedding(10004, 256)\n",
       "    )\n",
       "    (pos): PositionalEncoding()\n",
       "    (dec_blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (norm_1): LayerNormalization()\n",
       "        (norm_2): LayerNormalization()\n",
       "        (norm_3): LayerNormalization()\n",
       "        (ff): FeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (drop_1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_2): Dropout(p=0.1, inplace=False)\n",
       "        (drop_3): Dropout(p=0.1, inplace=False)\n",
       "        (att_1): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (att_2): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (norm_1): LayerNormalization()\n",
       "        (norm_2): LayerNormalization()\n",
       "        (norm_3): LayerNormalization()\n",
       "        (ff): FeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (drop_1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_2): Dropout(p=0.1, inplace=False)\n",
       "        (drop_3): Dropout(p=0.1, inplace=False)\n",
       "        (att_1): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (att_2): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (norm_1): LayerNormalization()\n",
       "        (norm_2): LayerNormalization()\n",
       "        (norm_3): LayerNormalization()\n",
       "        (ff): FeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (drop_1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_2): Dropout(p=0.1, inplace=False)\n",
       "        (drop_3): Dropout(p=0.1, inplace=False)\n",
       "        (att_1): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (att_2): MultiHeadAttention(\n",
       "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=10004, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(src_vocab_size, trg_vocab_size, D_MODEL)\n",
    "\n",
    "checkpoint = torch.load(model_path_root + \"2_5000\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(epoch)\n",
    "print(\"loss:\", loss)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66a9bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src, max_len = 80):\n",
    "    \n",
    "    model.eval()\n",
    "    input_pad = english.vocab.stoi['<pad>']\n",
    "    \n",
    "    #print(\"Here\")\n",
    "    src = tokenize_english(src)\n",
    "    sentence= torch.LongTensor([[english.vocab.stoi[tok] for tok\n",
    "    in src]])\n",
    "    src_mask = (sentence != input_pad).unsqueeze(-2)\n",
    "    e_outputs = model.encoder(sentence, src_mask)\n",
    "\n",
    "    outputs = torch.zeros(max_len, dtype=torch.long) #.type_as(src.data)\n",
    "    #print(type(outputs))\n",
    "    outputs[0] = torch.LongTensor([italian.vocab.stoi['<sos>']])\n",
    "        \n",
    "    for i in range(1, max_len):    \n",
    "\n",
    "        trg_mask = np.triu(np.ones((1, i, i)),k=1) #.astype('uint8')\n",
    "        trg_mask = torch.from_numpy(trg_mask) == 0\n",
    "\n",
    "        out = model.fc(model.decoder(outputs[:i].unsqueeze(0),\n",
    "        e_outputs, src_mask, trg_mask))\n",
    "        out = torch.nn.functional.softmax(out, dim=-1)\n",
    "        val, ix = out[:, -1].data.topk(1)\n",
    "\n",
    "        outputs[i] = ix[0][0]\n",
    "        if ix[0][0] == italian.vocab.stoi['<eos>']:\n",
    "            break\n",
    "    return ' '.join(\n",
    "        [italian.vocab.itos[ix] for ix in outputs[:i]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9128fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> non può essere d' accordo , questo è sbagliato <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print(translate(model, \"I cannot agree to that, this is wrong <eos>\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e73f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
